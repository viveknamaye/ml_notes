# ml_notes âœ¨
Machine Learning notes 

## CORE CONCEPTS : 

- Optimization Algorithm - gradient descent
- Cost Function - j(theta) 
- Hypothesis - h(x, y) 
- Algorithm - linear regression , logistic regression , etc. 
- Neural Networks - for non linear relationships

### First and foremost it is very important to know the difference in their roles in the working of a model.

<hr/>

Misconceptions cleared till now: 

- Gradient descent will always reach the global minima for convex functions like linear and logistic regression . The only problem is the rate of convergence.
- Gradient descent is not a hypothesis , rather it is a Optimization algorithm which is used to converge the loss function. Similar algorithms are available but have higher complexity.
- Difference between a normal logistic regression model and a neural network.
- Functioning of hidden layers in neural network. Basically to generate all the permutations and combinations of x^n terms in the hypothesis thus finding more complex relationships between the input features. Exactly which layer learn what still unknown to me.

<hr/>

This weeks goal : 
- Learn enough tensorflow to understand kaggle code
